# -*- coding: utf-8 -*-
"""credit_card_fraud_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qnVicXKvIo-_l1h4UI_1CqR0LlCHIHiO

# Credit Card Fraud Detection

## Overview

This project aims to identify credit card fraud using machine learning models. The dataset consists of transaction features, and the objective is to categorize transactions as either fraudulent or legitimate. Multiple machine learning models, including Random Forest, XGBoost, and Logistic Regression, will be utilized to detect fraudulent transactions and create visualizations.

## Machine Learning Models:
- Random Forest Classifier: An ensemble method that combines multiple decision trees to improve classification accuracy
- XGBoost Classifier: A gradient boosting technique that builds models sequentially to correct errors made by previous models
- Logistic Regression: A statistical method used for binary classification problems

Cited from the UC Irvine Machine Learning Repository, the three models were identified to produce the most accurate and precise predictions, compared to Support Vector and Neural Network classifications for a similar dataset.

## Step-by-step Process

1. Installation
2. Data Preparation
3. Model Training and Evaluation
4. Visualization


## Installation

Ensure you have the following Python packages installed:

- `numpy`
- `pandas`
- `matplotlib`
- `seaborn`
- `scikit-learn`
- `xgboost`

Sources:
- https://archive.ics.uci.edu/dataset/27/credit+approval
- https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud

## 1. Installation
"""

# Install all libraries
!pip install pandas
!pip install numpy
!pip install matplotlib
!pip install seaborn
!pip install scikit-learn
!pip install xgboost

print("All libraries installed")

# Import all libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, roc_curve, auc, precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
import xgboost as xgb
from xgboost import XGBClassifier

print("All libraries imported")

"""## 2. Data Preparation"""

# Load dataset
credit_card = pd.read_csv('creditcard.csv')
credit_card.dropna(inplace=True)
credit_card

"""## 3. Model Training and Evaluation

For this project, we'll use the Random Forest Classification, XG Boost Classification, and Logistic Regression machine learning models to determine which indexes are identified as a fraudulent transaction.
"""

# Identify features and target variable
X = credit_card.drop(['Amount', 'Class'], axis=1)
y = credit_card['Class']
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Train the models
xgb_model = XGBClassifier(eval_metric='logloss', random_state=42)
xgb_model.fit(X_train, y_train)

log_reg_model = LogisticRegression(solver='liblinear', random_state=42)
log_reg_model.fit(X_train, y_train)

rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
y_pred_xgb = xgb_model.predict(X_test)
y_pred_log_reg = log_reg_model.predict(X_test)
y_pred_rf = rf_model.predict(X_test)

# Compute and display metrics for model evaluation
def print_metrics(y_true, y_pred, model_name):
    print(f"Metrics for {model_name}:")
    print(f"Accuracy: {accuracy_score(y_true, y_pred):.4f}")
    print(f"Precision: {precision_score(y_true, y_pred):.4f}")
    print(f"Recall: {recall_score(y_true, y_pred):.4f}")
    print(f"F1 Score: {f1_score(y_true, y_pred):.4f}")
    print(f"ROC-AUC: {roc_auc_score(y_true, y_pred):.4f}")
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=['Normal', 'Anomaly']))
    print("\n")

print_metrics(y_test, y_pred_xgb, 'XGBoost')
print_metrics(y_test, y_pred_log_reg, 'Logistic Regression')
print_metrics(y_test, y_pred_rf, 'Random Forest')

"""Interpreted from these metrics, all three models (XGBoost, Logistic Regression, and Random Forest) perform exceptionally well with high accuracy, precision, recall, and F1 scores. The ROC-AUC score further reinforces their effectiveness in distinguishing between fraudulent and non-fraudulent transactions.

It's important to note that while precision and F1 scores are perfect, the recall of 81.82% shows that there's room for improvement in detecting all fraudulent transactions. This trade-off implies that reducing false positives can sometimes increase false negatives.

## 4. Visualization
"""

# Create confusion matrices
cm_xgb = confusion_matrix(y_test, y_pred_xgb)
cm_log_reg = confusion_matrix(y_test, y_pred_log_reg)
cm_rf = confusion_matrix(y_test, y_pred_rf)

fig, axs = plt.subplots(1, 3, figsize=(18, 5))

# XGBoost
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues', ax=axs[0])
axs[0].set_title('XGBoost Confusion Matrix')
axs[0].set_xlabel('Predicted')
axs[0].set_ylabel('True')

# Logistic Regression
sns.heatmap(cm_log_reg, annot=True, fmt='d', cmap='Blues', ax=axs[1])
axs[1].set_title('Logistic Regression Confusion Matrix')
axs[1].set_xlabel('Predicted')
axs[1].set_ylabel('True')

# Random Forest
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=axs[2])
axs[2].set_title('Random Forest Confusion Matrix')
axs[2].set_xlabel('Predicted')
axs[2].set_ylabel('True')

plt.show()

# Plot feature importances
plt.figure(figsize=(12, 8))
xgb.plot_importance(xgb_model, importance_type='weight')
plt.title('Feature Importance')
plt.show()

"""The Receiving Operator Curve plots the True Positive Rate (recall) against the False Positive Rate at various threshold settings. Models with ROC curves at the top-left corner of the chart indicate high TPR and low FPR, further emphasizing the validity of the predictions."""

# Plot ROC curve
fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_model.predict_proba(X_test)[:, 1])
fpr_log_reg, tpr_log_reg, _ = roc_curve(y_test, log_reg_model.predict_proba(X_test)[:, 1])
fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_model.predict_proba(X_test)[:, 1])

plt.figure(figsize=(10, 7))
plt.plot(fpr_xgb, tpr_xgb, color='blue', label='XGBoost (area = %0.2f)' % auc(fpr_xgb, tpr_xgb))
plt.plot(fpr_log_reg, tpr_log_reg, color='green', label='Logistic Regression (area = %0.2f)' % auc(fpr_log_reg, tpr_log_reg))
plt.plot(fpr_rf, tpr_rf, color='red', label='Random Forest (area = %0.2f)' % auc(fpr_rf, tpr_rf))
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc='lower right')
plt.show()

# Get anomaly probabilities
y_prob_xgb = xgb_model.predict_proba(X_test)[:, 1]
y_prob_log_reg = log_reg_model.predict_proba(X_test)[:, 1]
y_prob_rf = rf_model.predict_proba(X_test)[:, 1]
threshold = 0.5

# Identify outliers and extract detected rows from test data
outliers_xgb = y_prob_xgb > threshold
outliers_log_reg = y_prob_log_reg > threshold
outliers_rf = y_prob_rf > threshold
outliers_X_test_xgb = X_test[outliers_xgb]
outliers_X_test_log_reg = X_test[outliers_log_reg]
outliers_X_test_rf = X_test[outliers_rf]
outliers_y_test_xgb = y_test[outliers_xgb]
outliers_y_test_log_reg = y_test[outliers_log_reg]
outliers_y_test_rf = y_test[outliers_rf]
indices_outliers_xgb = np.where(outliers_xgb)[0]
indices_outliers_log_reg = np.where(outliers_log_reg)[0]
indices_outliers_rf = np.where(outliers_rf)[0]

# Retrieve rows corresponding to outliers
outliers_rows_xgb = X_test[indices_outliers_xgb]
outliers_rows_log_reg = X_test[indices_outliers_log_reg]
outliers_rows_rf = X_test[indices_outliers_rf]

# Print indices and first five sample rows
print(f"Indices of outliers detected by XGBoost: {indices_outliers_xgb}")
print("\n")
print("Sample outliers detected by XGBoost:")
print(outliers_rows_xgb[:5])
print("\n")
print(f"Indices of outliers detected by Logistic Regression: {indices_outliers_log_reg}")
print("\n")
print("Sample outliers detected by Logistic Regression:")
print(outliers_rows_log_reg[:5])
print("\n")
print(f"Indices of outliers detected by Random Forest: {indices_outliers_rf}")
print("\n")
print("Sample outliers detected by Random Forest:")
print(outliers_rows_rf[:5])

# Identify common instances in all models
combined_indices = np.concatenate([indices_outliers_xgb, indices_outliers_log_reg, indices_outliers_rf])
unique_indices = sorted(set(combined_indices))

print("Outlier indices classified by all three models:")
print(unique_indices)

# Create 2D Scatterplots visualizing normal and anomaly-detected instances
sns.set(style="whitegrid")
palette = sns.color_palette("Paired")

plt.figure(figsize=(20, 10))

# XGBoost
plt.subplot(1, 3, 1)
plt.scatter(X_test[~outliers_xgb, 0], X_test[~outliers_xgb, 1], c=palette[0], label='Normal')
plt.scatter(X_test[outliers_xgb, 0], X_test[outliers_xgb, 1], c=palette[5], label='Anomalies')

for i in np.where(outliers_xgb)[0]:
    plt.annotate(i, (X_test[i, 0], X_test[i, 1]), fontsize=8, alpha=0.7)

plt.title('XGBoost Anomalies')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()

# Logistic Regression
plt.subplot(1, 3, 2)
plt.scatter(X_test[~outliers_log_reg, 0], X_test[~outliers_log_reg, 1], c=palette[0], label='Normal')
plt.scatter(X_test[outliers_log_reg, 0], X_test[outliers_log_reg, 1], c=palette[5], label='Anomalies')

for i in np.where(outliers_log_reg)[0]:
    plt.annotate(i, (X_test[i, 0], X_test[i, 1]), fontsize=8, alpha=0.7)

plt.title('Logistic Regression Anomalies')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()

# Random Forest Scatter
plt.subplot(1, 3, 3)
plt.scatter(X_test[~outliers_rf, 0], X_test[~outliers_rf, 1], c=palette[0], label='Normal')
plt.scatter(X_test[outliers_rf, 0], X_test[outliers_rf, 1], c=palette[5], label='Anomalies')

for i in np.where(outliers_rf)[0]:
    plt.annotate(i, (X_test[i, 0], X_test[i, 1]), fontsize=8, alpha=0.7)

plt.title('Random Forest Anomalies')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.tight_layout()
plt.show()

# Apply PCA to reduce dimensions to 3D
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X_test)

# Create 3D Scatterplots visualizing normal and anomaly-detected instances
fig = plt.figure(figsize=(18, 6))

# XGBoost
ax1 = fig.add_subplot(131, projection='3d')
ax1.scatter(X_pca[outliers_xgb, 0], X_pca[outliers_xgb, 1], X_pca[outliers_xgb, 2], c='red', label='Anomalies')
ax1.scatter(X_pca[~outliers_xgb, 0], X_pca[~outliers_xgb, 1], X_pca[~outliers_xgb, 2], c='blue', label='Normal')
ax1.set_title('XGBoost Anomalies')
ax1.set_xlabel('PCA Component 1')
ax1.set_ylabel('PCA Component 2')
ax1.set_zlabel('PCA Component 3')
ax1.legend()

# Logistic Regression
ax2 = fig.add_subplot(132, projection='3d')
ax2.scatter(X_pca[outliers_log_reg, 0], X_pca[outliers_log_reg, 1], X_pca[outliers_log_reg, 2], c='red', label='Anomalies')
ax2.scatter(X_pca[~outliers_log_reg, 0], X_pca[~outliers_log_reg, 1], X_pca[~outliers_log_reg, 2], c='blue', label='Normal')
ax2.set_title('Logistic Regression Anomalies')
ax2.set_xlabel('PCA Component 1')
ax2.set_ylabel('PCA Component 2')
ax2.set_zlabel('PCA Component 3')
ax2.legend()

# Random Forest
ax3 = fig.add_subplot(133, projection='3d')
ax3.scatter(X_pca[outliers_rf, 0], X_pca[outliers_rf, 1], X_pca[outliers_rf, 2], c='red', label='Anomalies')
ax3.scatter(X_pca[~outliers_rf, 0], X_pca[~outliers_rf, 1], X_pca[~outliers_rf, 2], c='blue', label='Normal')
ax3.set_title('Random Forest Anomalies')
ax3.set_xlabel('PCA Component 1')
ax3.set_ylabel('PCA Component 2')
ax3.set_zlabel('PCA Component 3')
ax3.legend()

plt.tight_layout()
plt.show()

